{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load module and dataset\n",
    "'''\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#load dataset\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#create dictionary of vocab    \n",
    "vocab = sorted(set(text))\n",
    "#convert vocab into int\n",
    "vocabToInt = {char : i for i, char in enumerate(vocab)}\n",
    "#convert int to vocab\n",
    "intToVocab = dict(enumerate(vocab))\n",
    "#encode the character\n",
    "encoded = np.array([vocabToInt[c] for c in text], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "\n",
      "The encoded text above are:\n",
      "[31 64 57 72 76 61 74  1 16  0  0  0 36 57 72 72 81  1 62 57 69 65 68 65\n",
      " 61 75  1 57 74 61  1 57 68 68  1 57 68 65 67 61 26  1 61 78 61 74 81  1\n",
      " 77 70 64 57 72 72 81  1 62 57 69 65 68 81  1 65 75  1 77 70 64 57 72 72\n",
      " 81  1 65 70  1 65 76 75  1 71 79 70  0 79 57 81 13  0  0 33 78 61 74 81\n",
      " 76 64 65 70]\n",
      "Number of classes that our network has to pick from is 83\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print first 100 characters and print the encoded one\n",
    "'''\n",
    "print(text[ : 100])\n",
    "print('')\n",
    "print('The encoded text above are:')\n",
    "print('{}'.format(encoded[ : 100]))\n",
    "\n",
    "#classes that our network has\n",
    "print('Number of classes that our network has to pick from is {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure the data is shifted over one step for y\n",
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "making training mini-batches\n",
    "----------------------------\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches.\n",
    "Each batch contains NxM characters, where N is the batch size(the number of sequence) and M is the number of steps\n",
    "K is the number of the batches, so the total number of characters to keep from arr, N * M * K.\n",
    "\n",
    "The idea is each batch is a NxM window on the Nx(M*K) array, with each subsequent batch, the window moves over\n",
    "by n steps. Remember that the targets are the inputs shiffted over one character\n",
    "'''\n",
    "\n",
    "def getBatches(arr, batchSize, nSteps):\n",
    "    '''\n",
    "    create a generator that returns batches of size batchSize X nSteps from arr\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    arr: array you want to make batches from\n",
    "    batchSize: batch size, the number sequences per batch\n",
    "    nSteps: number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    #get the number of characters per batch and number of batches we can make\n",
    "    charsPerBatch = batchSize * nSteps\n",
    "    nBatch = len(arr) // charsPerBatch\n",
    "    \n",
    "    #keep only enough characters to make full batches\n",
    "    arr = arr[ : nBatch * charsPerBatch]\n",
    "    \n",
    "    #reshape into batchSize rows\n",
    "    arr = arr.reshape((batchSize, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], nSteps):\n",
    "        #the features\n",
    "        x = arr[ : , n : n + nSteps]\n",
    "        #the targets, shifted by one\n",
    "        yTemp = arr[ : ,n + 1 : n + nSteps + 1]\n",
    "        \n",
    "        '''\n",
    "        for the very last batch, y will be one characters short at the end of the sequences which break things.\n",
    "        To handle this, make an array of the appropriate size first, with all zeros then add the targets.\n",
    "        '''\n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "        y[ : , : yTemp.shape[1]] = yTemp\n",
    "        \n",
    "        yield x, y\n",
    "        \n",
    "#test the batches function with size of 10 and steps of 50\n",
    "batches = getBatches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('Make sure the data is shifted over one step for y')\n",
    "print('x\\n', x[ : 10, : 10])\n",
    "print('\\ny\\n', y[ : 10, : 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "building the model, using tensorflow\n",
    "'''\n",
    "def buildInputs(batchSize, numSteps):\n",
    "    '''\n",
    "    define placeholder for inputs, targets, and dropout\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    batchSize: batch size, number of sequences per batch\n",
    "    numSteps: number of sequence steps in a batch\n",
    "    '''\n",
    "    \n",
    "    #declare placeholders we'll feed into the graph\n",
    "    inputs  = tf.placeholder(tf.int32, [batchSize, numSteps], name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batchSize, numSteps], name = 'targets')\n",
    "    \n",
    "    #dropout probability\n",
    "    dropoutProb = tf.placeholder(tf.float32, name = 'dropoutProb')\n",
    "    \n",
    "    return inputs, targets, dropoutProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "build long short term memory\n",
    "The idea behind this is building a stack of LSTM cell\n",
    "but we still need to define LSTM cell\n",
    "'''\n",
    "\n",
    "def buildLSTM(LSTMSize, numLayer, batchSize, dropoutProb):\n",
    "    '''\n",
    "    build LSTM cell\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    LSTMSize: size of the hidden layer in the LSTM cells\n",
    "    numLayer: number of LSTM layers\n",
    "    batchSize: batch size\n",
    "    dropoutProb: scalar tensor(tf.placeholder) for the dropout prob\n",
    "    '''\n",
    "    \n",
    "    ##build the LSTM cell\n",
    "    def buildCell(LSTMSize, dropoutProb):\n",
    "        #use basic LSTM cell\n",
    "        LSTM = tf.contrib.rnn.BasicLSTMCell(LSTMSize) #basic cell\n",
    "        #add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(LSTM, output_keep_prob = dropoutProb)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    #stack LSTM cell\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([buildCell(LSTMSize, dropoutProb) for _ in range(numLayer)]) #stack of LSTM cell\n",
    "    intialState = cell.zero_state(batchSize, tf.float32)\n",
    "    \n",
    "    return cell, intialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RNN output\n",
    "-----------\n",
    "we need to connect the output of the RNN cells to a full connected layer with a softmax output\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs.\n",
    "We should reshape the output into a 2D tensor with shape (M*N) X L. That is 1 row for each sequence and step\n",
    "where the values of each row are the output from the LSTM cells\n",
    "'''\n",
    "\n",
    "def buildOutput(LSTMOutput, inSize, outSize):\n",
    "    '''\n",
    "    Build a softmax layer, return softmax output and logits\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    x: input tensor\n",
    "    inSize: size of the input tensor, size of the LSTM cells\n",
    "    outSize: size of the softmax layer\n",
    "    '''\n",
    "    \n",
    "    #reshape output so it's a buch of rows, one row for each step for each sequence\n",
    "    #the shape should be batchSize * numSteps rows by LSTM size columns\n",
    "    SequenceOutput = tf.concat(LSTMOutput, axis = 1)\n",
    "    x = tf.reshape(SequenceOutput, [-1, inSize]) #transpose the shape\n",
    "    \n",
    "    #connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmaxW = tf.Variable(tf.truncated_normal((inSize, outSize), stddev = 0.1))\n",
    "        softmaxB = tf.Variable(tf.zeros(outSize))\n",
    "        \n",
    "    #since output is bunch a rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logits output, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmaxW) + softmaxB\n",
    "    \n",
    "    #use softmax to get the probabilities for predicted characters\n",
    "    output = tf.nn.softmax(logits, name = 'predictions') # output of fully connected neural network\n",
    "    \n",
    "    return output, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "training loss\n",
    "-------------\n",
    "We need to one hot encode the targets, we get them as the encoded one\n",
    "then we reshape them into a 2D tensor with size of (M * N) x C where C is the number of classes\n",
    "'''\n",
    "\n",
    "def buildLoss(logits, targets, LSTMSize, numClasses):\n",
    "    '''\n",
    "    calculate the loss from the logits and the targets\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    logits: logits from final fully connected layer\n",
    "    targets: targets for supervised learning\n",
    "    LSTMSize: number of LSTM hidden units\n",
    "    numClasses: number of classes in targets\n",
    "    '''\n",
    "    \n",
    "    #one hot encode targets and reshape to match logits, one row per batchSize per step\n",
    "    yOneHot = tf.one_hot(targets, numClasses)\n",
    "    yReshaped = tf.reshape(yOneHot, logits.get_shape())\n",
    "    \n",
    "    #softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = yReshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "optimizer for gradient exploding and disappearing problem\n",
    "---------------------------------------------------------\n",
    "Using gradient clip above some threshold. If a gradient is larger than a threshold, we set it to the threshold\n",
    "this will ensure the gradients never grow overly large\n",
    "'''\n",
    "\n",
    "def buildOptimizer(loss, learningRate, gradClip):\n",
    "    '''\n",
    "    build optimizer for training, using gradient clipping\n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    loss: network loss\n",
    "    learningRate: learning rate for optimizer\n",
    "    '''\n",
    "    \n",
    "    #optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tVars = tf.trainable_variables()\n",
    "    grad, _ = tf.clip_by_global_norm(tf.gradients(loss, tVars), gradClip)\n",
    "    trainOp = tf.train.AdamOptimizer(learningRate)\n",
    "    optimizer = trainOp.apply_gradients(zip(grad, tVars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "build network\n",
    "-------------\n",
    "create a class for the RNN. The is function will pass the hidden and cell states across LSTM cells appropriately\n",
    "it returns the output for each LSTM cell at each step for each sequence in the mini-batch and also give the LSTM state\n",
    "\n",
    "This class is using all function created before.\n",
    "'''\n",
    "\n",
    "class charRNN:\n",
    "    def __init__(self, numClasses, batchSize = 64, numSteps = 50, \n",
    "                       LSTMSize = 128, numLayer = 2, learningRate = 0.0001, \n",
    "                       gradClip = 5, sampling = False):\n",
    "        \n",
    "        #when we are using the network for sampling, we will passing in one character at a time\n",
    "        if sampling == True:\n",
    "            batchSize, numSteps = 1, 1\n",
    "        else:\n",
    "            batchSize, numSteps = batchSize, numSteps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #build the input placeholder tensor\n",
    "        self.inputs, self.targets, self.dropoutProb = buildInputs(batchSize, numSteps) #buildInputs function created above\n",
    "        #buidl the LSTM cell\n",
    "        cell, self.initialState, = buildLSTM(LSTMSize, numLayer, batchSize, self.dropoutProb) #build LSTM function created above\n",
    "        \n",
    "        ##run the date through the RNN layers\n",
    "        #one hot encode the input tokens\n",
    "        xOneHot = tf.one_hot(self.inputs, numClasses)\n",
    "        \n",
    "        #run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, xOneHot, initial_state = self.initialState)\n",
    "        self.finalState = state\n",
    "        \n",
    "        #get sofmax predictions and logits\n",
    "        self.prediction, self.logits = buildOutput(outputs, LSTMSize, numClasses) #buildOutput function created above\n",
    "        \n",
    "        #loss and optimizer with gradient clipping \n",
    "        self.loss = buildLoss(self.logits, self.targets, LSTMSize, numClasses) #buildLoss function created above\n",
    "        self.optimizer = buildOptimizer(self.loss, learningRate, gradClip) #buildOptimizer function created above\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "hyperparameter\n",
    "--------------\n",
    "written separately in purpose to make it simpler to understand\n",
    "'''\n",
    "\n",
    "batchSize = 100 #sequence per batch\n",
    "numSteps = 100 #number of sequence steps per batch\n",
    "LSTMSize = 512 #size of hidden layer in LSTM\n",
    "numLayer = 2 #number of LSTM layer\n",
    "learningRate = 0.001 #learning rate\n",
    "dropoutProb = 0.5 #dropout probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-1e0e57634a88>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "Epochs: 1/20... Training step: 50... Training loss: 1.0613077878952026... 0.3165sec/batch\n",
      "Epochs: 1/20... Training step: 100... Training loss: 1.023555874824524... 0.3196sec/batch\n",
      "Epochs: 1/20... Training step: 150... Training loss: 1.033983588218689... 0.3199sec/batch\n",
      "Epochs: 2/20... Training step: 200... Training loss: 1.0533851385116577... 0.3191sec/batch\n",
      "Epochs: 2/20... Training step: 250... Training loss: 1.0888053178787231... 0.3157sec/batch\n",
      "Epochs: 2/20... Training step: 300... Training loss: 1.0326857566833496... 0.3183sec/batch\n",
      "Epochs: 2/20... Training step: 350... Training loss: 1.0351042747497559... 0.3182sec/batch\n",
      "Epochs: 3/20... Training step: 400... Training loss: 1.0727630853652954... 0.3194sec/batch\n",
      "Epochs: 3/20... Training step: 450... Training loss: 1.0485177040100098... 0.3186sec/batch\n",
      "Epochs: 3/20... Training step: 500... Training loss: 1.0297517776489258... 0.3215sec/batch\n",
      "Epochs: 3/20... Training step: 550... Training loss: 1.0416229963302612... 0.3172sec/batch\n",
      "Epochs: 4/20... Training step: 600... Training loss: 1.0195839405059814... 0.3168sec/batch\n",
      "Epochs: 4/20... Training step: 650... Training loss: 1.043803095817566... 0.3180sec/batch\n",
      "Epochs: 4/20... Training step: 700... Training loss: 1.0257657766342163... 0.3212sec/batch\n",
      "Epochs: 4/20... Training step: 750... Training loss: 1.0388696193695068... 0.3190sec/batch\n",
      "Epochs: 5/20... Training step: 800... Training loss: 1.0373663902282715... 0.3187sec/batch\n",
      "Epochs: 5/20... Training step: 850... Training loss: 1.0378191471099854... 0.3179sec/batch\n",
      "Epochs: 5/20... Training step: 900... Training loss: 1.0381344556808472... 0.3187sec/batch\n",
      "Epochs: 5/20... Training step: 950... Training loss: 1.0360379219055176... 0.3202sec/batch\n",
      "Epochs: 6/20... Training step: 1000... Training loss: 1.038834810256958... 0.3197sec/batch\n",
      "Epochs: 6/20... Training step: 1050... Training loss: 1.0701711177825928... 0.3203sec/batch\n",
      "Epochs: 6/20... Training step: 1100... Training loss: 1.0425564050674438... 0.3176sec/batch\n",
      "Epochs: 6/20... Training step: 1150... Training loss: 1.04762601852417... 0.3178sec/batch\n",
      "Epochs: 7/20... Training step: 1200... Training loss: 1.0380407571792603... 0.3167sec/batch\n",
      "Epochs: 7/20... Training step: 1250... Training loss: 1.0711921453475952... 0.3194sec/batch\n",
      "Epochs: 7/20... Training step: 1300... Training loss: 1.035478949546814... 0.3158sec/batch\n",
      "Epochs: 7/20... Training step: 1350... Training loss: 1.0458656549453735... 0.3160sec/batch\n",
      "Epochs: 8/20... Training step: 1400... Training loss: 1.0377357006072998... 0.3192sec/batch\n",
      "Epochs: 8/20... Training step: 1450... Training loss: 1.0482035875320435... 0.3174sec/batch\n",
      "Epochs: 8/20... Training step: 1500... Training loss: 1.0246129035949707... 0.3183sec/batch\n",
      "Epochs: 8/20... Training step: 1550... Training loss: 1.0351966619491577... 0.3153sec/batch\n",
      "Epochs: 9/20... Training step: 1600... Training loss: 1.0160382986068726... 0.3185sec/batch\n",
      "Epochs: 9/20... Training step: 1650... Training loss: 1.0381792783737183... 0.3144sec/batch\n",
      "Epochs: 9/20... Training step: 1700... Training loss: 1.001590371131897... 0.3178sec/batch\n",
      "Epochs: 9/20... Training step: 1750... Training loss: 1.0335721969604492... 0.3184sec/batch\n",
      "Epochs: 10/20... Training step: 1800... Training loss: 1.0505017042160034... 0.3213sec/batch\n",
      "Epochs: 10/20... Training step: 1850... Training loss: 1.019105076789856... 0.3161sec/batch\n",
      "Epochs: 10/20... Training step: 1900... Training loss: 1.0264678001403809... 0.3169sec/batch\n",
      "Epochs: 10/20... Training step: 1950... Training loss: 1.0696706771850586... 0.3194sec/batch\n",
      "Epochs: 11/20... Training step: 2000... Training loss: 1.0461345911026... 0.3202sec/batch\n",
      "Epochs: 11/20... Training step: 2050... Training loss: 1.0170378684997559... 0.3184sec/batch\n",
      "Epochs: 11/20... Training step: 2100... Training loss: 1.0311671495437622... 0.3199sec/batch\n",
      "Epochs: 11/20... Training step: 2150... Training loss: 1.0356053113937378... 0.3196sec/batch\n",
      "Epochs: 12/20... Training step: 2200... Training loss: 1.0297677516937256... 0.3200sec/batch\n",
      "Epochs: 12/20... Training step: 2250... Training loss: 1.0395764112472534... 0.3180sec/batch\n",
      "Epochs: 12/20... Training step: 2300... Training loss: 0.9961411952972412... 0.3190sec/batch\n",
      "Epochs: 12/20... Training step: 2350... Training loss: 1.0120532512664795... 0.3186sec/batch\n",
      "Epochs: 13/20... Training step: 2400... Training loss: 1.0412559509277344... 0.3167sec/batch\n",
      "Epochs: 13/20... Training step: 2450... Training loss: 1.0013052225112915... 0.3195sec/batch\n",
      "Epochs: 13/20... Training step: 2500... Training loss: 1.0107721090316772... 0.3172sec/batch\n",
      "Epochs: 13/20... Training step: 2550... Training loss: 1.0258172750473022... 0.3190sec/batch\n",
      "Epochs: 14/20... Training step: 2600... Training loss: 0.992441713809967... 0.3186sec/batch\n",
      "Epochs: 14/20... Training step: 2650... Training loss: 1.021417260169983... 0.3172sec/batch\n",
      "Epochs: 14/20... Training step: 2700... Training loss: 0.9801216721534729... 0.3197sec/batch\n",
      "Epochs: 14/20... Training step: 2750... Training loss: 1.0048638582229614... 0.3191sec/batch\n",
      "Epochs: 15/20... Training step: 2800... Training loss: 1.0347309112548828... 0.3208sec/batch\n",
      "Epochs: 15/20... Training step: 2850... Training loss: 1.023807168006897... 0.3187sec/batch\n",
      "Epochs: 15/20... Training step: 2900... Training loss: 1.006063461303711... 0.3199sec/batch\n",
      "Epochs: 15/20... Training step: 2950... Training loss: 1.0473592281341553... 0.3167sec/batch\n",
      "Epochs: 16/20... Training step: 3000... Training loss: 1.0213353633880615... 0.3171sec/batch\n",
      "Epochs: 16/20... Training step: 3050... Training loss: 1.02005934715271... 0.3166sec/batch\n",
      "Epochs: 16/20... Training step: 3100... Training loss: 0.982688844203949... 0.3185sec/batch\n",
      "Epochs: 16/20... Training step: 3150... Training loss: 0.9900578856468201... 0.3221sec/batch\n",
      "Epochs: 17/20... Training step: 3200... Training loss: 0.9854533076286316... 0.3197sec/batch\n",
      "Epochs: 17/20... Training step: 3250... Training loss: 1.0143415927886963... 0.3204sec/batch\n",
      "Epochs: 17/20... Training step: 3300... Training loss: 0.9982651472091675... 0.3174sec/batch\n",
      "Epochs: 17/20... Training step: 3350... Training loss: 1.0194100141525269... 0.3170sec/batch\n",
      "Epochs: 18/20... Training step: 3400... Training loss: 1.010693907737732... 0.3191sec/batch\n",
      "Epochs: 18/20... Training step: 3450... Training loss: 1.0027254819869995... 0.3196sec/batch\n",
      "Epochs: 18/20... Training step: 3500... Training loss: 0.9999070167541504... 0.3191sec/batch\n",
      "Epochs: 18/20... Training step: 3550... Training loss: 1.006949782371521... 0.3184sec/batch\n",
      "Epochs: 19/20... Training step: 3600... Training loss: 1.0006953477859497... 0.3182sec/batch\n",
      "Epochs: 19/20... Training step: 3650... Training loss: 0.9905723929405212... 0.3186sec/batch\n",
      "Epochs: 19/20... Training step: 3700... Training loss: 1.0064884424209595... 0.3196sec/batch\n",
      "Epochs: 19/20... Training step: 3750... Training loss: 0.989224910736084... 0.3171sec/batch\n",
      "Epochs: 20/20... Training step: 3800... Training loss: 0.9749049544334412... 0.3134sec/batch\n",
      "Epochs: 20/20... Training step: 3850... Training loss: 0.9908424019813538... 0.3211sec/batch\n",
      "Epochs: 20/20... Training step: 3900... Training loss: 1.0286452770233154... 0.3175sec/batch\n",
      "Epochs: 20/20... Training step: 3950... Training loss: 0.983415424823761... 0.3185sec/batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training the network\n",
    "'''\n",
    "epochs = 20\n",
    "printEveryN = 50\n",
    "saveEveryN = 200\n",
    "\n",
    "#create the object RNN network based on charRNN class\n",
    "model = charRNN(len(vocab), batchSize = batchSize, numSteps = numSteps, \n",
    "               LSTMSize = LSTMSize, numLayer = numLayer, learningRate = learningRate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0\n",
    "    \n",
    "    #load saved checkpoint\n",
    "    saver.restore(sess, 'checkpoints/i3960_l512.ckpt')\n",
    "    for e in range(epochs):\n",
    "        newState = sess.run(model.initialState)\n",
    "        loss = 0\n",
    "        \n",
    "        for x, y, in getBatches(encoded, batchSize, numSteps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x, \n",
    "                    model.targets: y, \n",
    "                    model.dropoutProb: dropoutProb, \n",
    "                    model.initialState: newState}\n",
    "            \n",
    "            batchLoss, newState, _ = sess.run([model.loss,\n",
    "                                               model.finalState, \n",
    "                                               model.optimizer], \n",
    "                                              feed_dict = feed)\n",
    "            \n",
    "            if(counter % printEveryN == 0):\n",
    "                end = time.time()\n",
    "                print('Epochs: {}/{}...'.format(e + 1, epochs), \n",
    "                      'Training step: {}...'.format(counter), \n",
    "                      'Training loss: {}...'.format(batchLoss), \n",
    "                      '{:.4f}sec/batch'.format((end - start)))\n",
    "                \n",
    "            if(counter % saveEveryN == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, LSTMSize))\n",
    "            \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, LSTMSize))\n",
    "    \n",
    "#saved checkpoint\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sampling\n",
    "--------\n",
    "The idea is that we pass in a character, then the network will predict the next character.\n",
    "We can use the new one to predict the next one\n",
    "'''\n",
    "\n",
    "def pickTopN(preds, vocabSize, topN = 5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p) [ : -topN]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocabSize, 1, p = p)[0]\n",
    "    \n",
    "    return c\n",
    "\n",
    "def sample(checkpoint, nSamples, LSTMSize, vocabSize, prime = \"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = charRNN(len(vocab), LSTMSize = LSTMSize, sampling = True)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        newState = sess.run(model.initialState)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocabToInt[c]\n",
    "            feed = {model.inputs: x, \n",
    "                    model.dropoutProb: 1., \n",
    "                    model.initialState: newState}\n",
    "            \n",
    "            preds, newState = sess.run([model.prediction, model.finalState], \n",
    "                                       feed_dict = feed)\n",
    "            \n",
    "        c = pickTopN(preds, len(vocab))\n",
    "        samples.append(intToVocab[c])\n",
    "        \n",
    "        for i in range(nSamples):\n",
    "            x[0, 0] = c\n",
    "            feed = {model.inputs: x, \n",
    "                    model.dropoutProb: 1., \n",
    "                    model.initialState: newState}\n",
    "            \n",
    "            preds, newState = sess.run([model.prediction, model.finalState], \n",
    "                                       feed_dict = feed)\n",
    "            \n",
    "            c = pickTopN(preds, len(vocab))\n",
    "            samples.append(intToVocab[c])\n",
    "            \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "testing\n",
    "'''\n",
    "tf.train.latest_checkpoint('checkpoints')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to test the trained model to artificially predict sequence of the words within. The network will receive an incomplete phase and try to predict the sequence outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sampling, we try to predict the sequence of '''part''' words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "predicted 20 sequence of characters based on words given is: \n",
      " \n",
      "partly, who had been\n",
      "take\n"
     ]
    }
   ],
   "source": [
    "#sampling 1\n",
    "print(\"First sampling, we try to predict the sequence of '''part''' words\")\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 20, LSTMSize, len(vocab), prime = \"part\")\n",
    "print(\"predicted 20 sequence of characters based on words given is: \")\n",
    "print(' ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sampling, we try to predict the sequence of '''bec''' words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "predicted 20 sequence of characters based on words given is: \n",
      " \n",
      "become of the world. He \n"
     ]
    }
   ],
   "source": [
    "#sampling 2 \n",
    "print(\"First sampling, we try to predict the sequence of '''bec''' words\")\n",
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 20, LSTMSize, len(vocab), prime = \"bec\")\n",
    "print(\"predicted 20 sequence of characters based on words given is: \")\n",
    "print(' ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sampling, we try to predict the sequence of '''fas''' words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n",
      "predicted 20 sequence of characters based on words given is: \n",
      " \n",
      "fast that he saw in her\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sampling 3\n",
    "print(\"First sampling, we try to predict the sequence of '''fas''' words\")\n",
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 20, LSTMSize, len(vocab), prime = \"fas\")\n",
    "print(\"predicted 20 sequence of characters based on words given is: \")\n",
    "print(' ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sampling, we try to predict the sequence of '''Far''' words\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i1200_l512.ckpt\n",
      "predicted 20 sequence of characters based on words given is: \n",
      " \n",
      "Farritaly. He was not at\n"
     ]
    }
   ],
   "source": [
    "#sampling 4\n",
    "print(\"First sampling, we try to predict the sequence of '''Far''' words\")\n",
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 20, LSTMSize, len(vocab), prime = \"Far\")\n",
    "print(\"predicted 20 sequence of characters based on words given is: \")\n",
    "print(' ')\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
